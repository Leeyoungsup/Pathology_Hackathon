{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision.utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "import torchmetrics\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=1\n",
    "image_count=200\n",
    "tf = ToTensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gil/gcubme_ai2/Workspace/YS_Lee/어반_헤커톤/code/Pathology_Hackathon/tile200_patient1_classification.ipynb 셀 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/%EC%96%B4%EB%B0%98_%ED%97%A4%EC%BB%A4%ED%86%A4/code/Pathology_Hackathon/tile200_patient1_classification.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/%EC%96%B4%EB%B0%98_%ED%97%A4%EC%BB%A4%ED%86%A4/code/Pathology_Hackathon/tile200_patient1_classification.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m image_index:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/%EC%96%B4%EB%B0%98_%ED%97%A4%EC%BB%A4%ED%86%A4/code/Pathology_Hackathon/tile200_patient1_classification.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m     image\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\u001b[39m-\u001b[39mtf(Image\u001b[39m.\u001b[39;49mopen(train_image_file_list[index])\u001b[39m.\u001b[39;49mresize((\u001b[39m224\u001b[39;49m,\u001b[39m224\u001b[39;49m)))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/%EC%96%B4%EB%B0%98_%ED%97%A4%EC%BB%A4%ED%86%A4/code/Pathology_Hackathon/tile200_patient1_classification.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     train_image_tensor[i,count]\u001b[39m=\u001b[39mimage\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/%EC%96%B4%EB%B0%98_%ED%97%A4%EC%BB%A4%ED%86%A4/code/Pathology_Hackathon/tile200_patient1_classification.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     count\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/PIL/Image.py:2156\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2152\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   2154\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(size)\n\u001b[0;32m-> 2156\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   2157\u001b[0m \u001b[39mif\u001b[39;00m box \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2158\u001b[0m     box \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/PIL/ImageFile.py:283\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_end()\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exclusive_fp \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_exclusive_fp_after_loading:\n\u001b[0;32m--> 283\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mclose()\n\u001b[1;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m LOAD_TRUNCATED_IMAGES \u001b[39mand\u001b[39;00m err_code \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    287\u001b[0m     \u001b[39m# still raised if decoder fails to return anything\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_path,label):\n",
    "        self.img_path=image_path\n",
    "        self.label=label\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_tensor=self.img_path[idx]\n",
    "        label_tensor=self.label[idx]\n",
    "        return image_tensor,label_tensor\n",
    "    \n",
    "train_image_transition_path='../../data/Tiling/512/train/transition/*'\n",
    "train_image_not_transition_path='../../data/Tiling/512/train/not_transition/*'\n",
    "test_image_transition_path='../../data/Tiling/512/validation/transition/*'\n",
    "test_image_not_transition_path='../../data/Tiling/512/validation/not_transition/*'\n",
    "train_image_list=[]\n",
    "train_label_list=[]\n",
    "test_image_list=[]\n",
    "test_label_list=[]\n",
    "train_image_transition_list=glob(train_image_transition_path)\n",
    "train_image_transition_label=torch.ones(len(train_image_transition_list),1)\n",
    "train_image_not_transition_list=glob(train_image_not_transition_path)\n",
    "train_image_not_transition_label=torch.zeros(len(train_image_not_transition_list),1)\n",
    "train_image_list.extend(train_image_transition_list)\n",
    "train_image_list.extend(train_image_not_transition_list)\n",
    "train_label_list.extend(train_image_transition_label)\n",
    "train_label_list.extend(train_image_not_transition_label)\n",
    "test_image_transition_list=glob(test_image_transition_path)\n",
    "test_image_transition_label=torch.ones(len(test_image_transition_list),1)\n",
    "test_image_not_transition_list=glob(test_image_not_transition_path)\n",
    "test_image_not_transition_label=torch.zeros(len(test_image_not_transition_list),1)\n",
    "test_image_list.extend(test_image_transition_list)\n",
    "test_image_list.extend(test_image_not_transition_list)\n",
    "test_label_list.extend(test_image_transition_label)\n",
    "test_label_list.extend(test_image_not_transition_label)\n",
    "\n",
    "train_image_tensor=torch.empty((len(train_image_list),image_count,1,3,224,224))\n",
    "for i in range(len(train_image_list)):\n",
    "    train_image_file_list=glob(train_image_list[i]+'/*.jpg')\n",
    "    image_index=torch.randint(low=0, high=len(train_image_file_list),size=(image_count,))\n",
    "    count=0\n",
    "    for index in image_index:\n",
    "        image=1-tf(Image.open(train_image_file_list[index]).resize((224,224))).unsqueeze(0)\n",
    "        train_image_tensor[i,count]=image.unsqueeze(0)\n",
    "        count+=1\n",
    "\n",
    "test_image_tensor=torch.empty((len(test_image_list),image_count,1,3,224,224))\n",
    "for i in range(len(test_image_list)):\n",
    "    test_image_file_list=glob(test_image_list[i]+'/*.jpg')\n",
    "    image_index=torch.randint(low=0, high=len(test_image_file_list),size=(image_count,))\n",
    "    count=0\n",
    "    for index in image_index:\n",
    "        image=1-tf(Image.open(test_image_file_list[index]).resize((224,224))).unsqueeze(0)\n",
    "        test_image_tensor[i,count]=image.unsqueeze(0)\n",
    "        count+=1\n",
    "    \n",
    "train_dataset=CustomDataset(train_image_tensor,train_label_list)\n",
    "test_dataset=CustomDataset(test_image_tensor ,test_label_list)\n",
    "dataset_size = len(test_dataset)\n",
    "test_size = int(dataset_size * 0.5)\n",
    "validation_size = dataset_size-test_size\n",
    "validation_dataset, test_dataset = random_split(test_dataset, [validation_size, test_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_fn(x):\n",
    "    \"\"\" Swish activation function \"\"\"\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "class Conv2dSamePadding(nn.Conv2d):\n",
    "    \"\"\" 2D Convolutions like TensorFlow \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ih, iw = x.size()[-2:]\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, [pad_w//2, pad_w - pad_w//2, pad_h//2, pad_h - pad_h//2])\n",
    "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "    \n",
    "def drop_connect(inputs, p, training):\n",
    "    \"\"\" Drop connect. \"\"\"\n",
    "    if not training: return inputs\n",
    "    batch_size = inputs.shape[0]\n",
    "    keep_prob = 1 - p\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype)  # uniform [0,1)\n",
    "    binary_tensor = torch.floor(random_tensor)\n",
    "    output = inputs.to(device) / keep_prob * binary_tensor.to(device)\n",
    "    return output\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Mobile Inverted Residual Bottleneck Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride, expand_ratio, input_filters, output_filters, se_ratio, drop_n_add):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._bn_mom = 0.1\n",
    "        self._bn_eps = 1e-03\n",
    "        self.has_se = (se_ratio is not None) and (0 < se_ratio <= 1)\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.drop_n_add = drop_n_add\n",
    "\n",
    "        # Filter Expansion phase\n",
    "        inp = input_filters  # number of input channels\n",
    "        oup = input_filters * expand_ratio  # number of output channels\n",
    "        if expand_ratio != 1: # add it except at first block \n",
    "            self._expand_conv = Conv2dSamePadding(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
    "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        k = kernel_size\n",
    "        s = stride\n",
    "        self._depthwise_conv = Conv2dSamePadding(\n",
    "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise(conv filter by filter)\n",
    "            kernel_size=k, stride=s, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Squeeze and Excitation layer, if desired\n",
    "        if self.has_se:\n",
    "            num_squeezed_channels = max(1,int(input_filters * se_ratio))  # input channel * 0.25 ex) block2 => 16 * 0.25 = 4\n",
    "            self._se_reduce = Conv2dSamePadding(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
    "            self._se_expand = Conv2dSamePadding(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
    "\n",
    "        # Output phase\n",
    "        final_oup = output_filters\n",
    "        self._project_conv = Conv2dSamePadding(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
    "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "    def forward(self, inputs, drop_connect_rate=0.2):\n",
    "    \n",
    "        # Expansion and Depthwise Convolution\n",
    "        x = inputs\n",
    "        if self.expand_ratio != 1:\n",
    "            x = relu_fn(self._bn0(self._expand_conv(inputs)))\n",
    "        x = relu_fn(self._bn1(self._depthwise_conv(x)))\n",
    "\n",
    "        # Squeeze and Excitation\n",
    "        if self.has_se:\n",
    "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
    "            x_squeezed = self._se_expand(relu_fn(self._se_reduce(x_squeezed)))\n",
    "            x = torch.sigmoid(x_squeezed) * x\n",
    "            \n",
    "        # Output phase\n",
    "        x = self._bn2(self._project_conv(x))\n",
    "\n",
    "        # Skip connection and drop connect\n",
    "        if self.drop_n_add == True:\n",
    "            if drop_connect_rate:\n",
    "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
    "            x = x + inputs  # skip connection\n",
    "        return x    \n",
    "    \n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Batch norm parameters\n",
    "        bn_mom = 0.1\n",
    "        bn_eps = 1e-03\n",
    "\n",
    "        # stem\n",
    "        in_channels = 3\n",
    "        out_channels = 32\n",
    "        self._conv_stem = Conv2dSamePadding(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
    "        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Build blocks\n",
    "        self._blocks = nn.ModuleList([]) # list 형태로 model 구성할 때\n",
    "        # stage2 r1_k3_s11_e1_i32_o16_se0.25\n",
    "        self._blocks.append(MBConvBlock(kernel_size=3, stride=1, expand_ratio=1, input_filters=32, output_filters=16, se_ratio=0.25, drop_n_add=False))\n",
    "        # stage3 r2_k3_s22_e6_i16_o24_se0.25\n",
    "        self._blocks.append(MBConvBlock(3, 2, 6, 16, 24, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 24, 24, 0.25, True))\n",
    "        # stage4 r2_k5_s22_e6_i24_o40_se0.25\n",
    "        self._blocks.append(MBConvBlock(5, 2, 6, 24, 40, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 40, 40, 0.25, True))\n",
    "        # stage5 r3_k3_s22_e6_i40_o80_se0.25\n",
    "        self._blocks.append(MBConvBlock(3, 2, 6, 40, 80, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 80, 80, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 80, 80, 0.25, True))\n",
    "        # stage6 r3_k5_s11_e6_i80_o112_se0.25\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 80,  112, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 112, 112, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 112, 112, 0.25, True))\n",
    "        # stage7 r4_k5_s22_e6_i112_o192_se0.25\n",
    "        self._blocks.append(MBConvBlock(5, 2, 6, 112, 192, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 192, 192, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 192, 192, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 192, 192, 0.25, True))\n",
    "        # stage8 r1_k3_s11_e6_i192_o320_se0.25\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 192, 320, 0.25, False))\n",
    "\n",
    "        # Head \n",
    "        in_channels = 320\n",
    "        out_channels = 1280\n",
    "        self._conv_head = Conv2dSamePadding(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Final linear layer\n",
    "        self._dropout = 0.2\n",
    "        self._num_classes = 10\n",
    "        self._fc = nn.Linear(out_channels, self._num_classes)\n",
    "        self.fc1=nn.Linear(15680, 1000, bias=False)\n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
    "\n",
    "        # Stem\n",
    "        x = relu_fn(self._bn0(self._conv_stem(inputs)))\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self._blocks):          \n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n",
    "\n",
    "        # Convolution layers\n",
    "        x = self.extract_features(x.to(device)).to(device)\n",
    "        x = torch.flatten(x, 1).to(device)\n",
    "        x=self.fc1(x).to(device)\n",
    "        return x\n",
    "class Custom_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn=EfficientNet().to(device)\n",
    "        self.hidden_size = 128\n",
    "        self.num_layers = 2\n",
    "        self.rnn = nn.RNN(1000, 128, 2, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "    def forward(self, inputs):\n",
    "        total_x=torch.empty((batch_size,image_count,1000)).to(device)\n",
    "        for i in range(len(inputs)):\n",
    "            for j in range(len(inputs[i])):\n",
    "                x=self.cnn(inputs[i,j].to(device)).to(device)\n",
    "                total_x[i,j]=x.to(device)\n",
    "        h0 = torch.zeros(self.num_layers,total_x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.rnn(total_x, h0)\n",
    "\n",
    "        # 마지막 시퀀스의 출력을 사용하여 예측\n",
    "        out = self.fc(out[:, -1, :]).to(device)\n",
    "       \n",
    "        return out\n",
    "accuracy = torchmetrics.Accuracy(task=\"binary\", num_classes=1).to(device)\n",
    "model = Custom_model()\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "summary(model,(batch_size,image_count,1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_loss=5000\n",
    "train_loss_list=[]\n",
    "val_loss_list=[]\n",
    "train_acc_list=[]\n",
    "val_acc_list=[]\n",
    "for epoch in range(1000):\n",
    "    train=tqdm(train_dataloader)\n",
    "    count=0\n",
    "    running_loss = 0.0\n",
    "    acc_loss=0\n",
    "    for x, y in train:\n",
    "        model.train()\n",
    "        y = y.to(device).float()\n",
    "        count+=1\n",
    "        x=x.to(device).float()\n",
    "        optimizer.zero_grad()  # optimizer zero 로 초기화\n",
    "        predict = model(x).to(device)\n",
    "        cost = criterion(predict, y) # cost 구함\n",
    "        acc=accuracy(predict, y)\n",
    "        cost.backward() # cost에 대한 backward 구함\n",
    "        optimizer.step() \n",
    "        running_loss += cost.item()\n",
    "        acc_loss+=acc\n",
    "        train.set_description(f\"epoch: {epoch+1}/{1000} Step: {count+1} loss : {running_loss/count:.4f} accuracy: {acc_loss/count:.4f}\")\n",
    "    train_loss_list.append((running_loss/count))\n",
    "    train_acc_list.append((acc_loss/count).cpu().detach().numpy())\n",
    "#validation\n",
    "    val=tqdm(validation_dataloader)\n",
    "    model.eval()\n",
    "    count=0\n",
    "    val_running_loss=0.0\n",
    "    acc_loss=0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val:\n",
    "            y = y.to(device).float()\n",
    "            count+=1\n",
    "            x=x.to(device).float()\n",
    "            \n",
    "            predict = model(x).to(device)\n",
    "            acc=accuracy(predict, y)\n",
    "            cost = criterion(predict, y)\n",
    "            val_running_loss+=cost.item()\n",
    "            acc_loss+=acc\n",
    "            val.set_description(f\"Validation epoch: {epoch+1}/{1000} Step: {count+1} loss : {val_running_loss/count:.4f}  accuracy: {acc_loss/count:.4f}\")\n",
    "        val_loss_list.append((val_running_loss/count))\n",
    "        val_acc_list.append((acc_loss/count).cpu().detach().numpy())\n",
    "    if epoch%100==1:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.subplot(1, 2, 1) \n",
    "        plt.title('loss_graph')\n",
    "        plt.plot(np.arange(epoch+1),train_loss_list,label='train_loss')\n",
    "        plt.plot(np.arange(epoch+1),val_loss_list,label='validation_loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.ylim([0, 1]) \n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)  \n",
    "        plt.title('acc_graph')\n",
    "        plt.plot(np.arange(epoch+1),train_acc_list,label='train_acc')\n",
    "        plt.plot(np.arange(epoch+1),val_acc_list,label='validation_acc')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.ylim([0, 1]) \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    if MIN_loss>(val_running_loss/count):\n",
    "        torch.save(model.state_dict(), '../../model/image_512/EffB0_tile200_RNN_callback.pt')\n",
    "        MIN_loss=(val_running_loss/count)\n",
    "torch.save(model.state_dict(), '../../model/image_512/EffB0_tile200_RNN.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "import torch.nn as nn\n",
    "import torchvision.models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision.utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from copy import copy\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "import torchmetrics\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size=1\n",
    "image_count=200\n",
    "tf = ToTensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_path,label):\n",
    "        self.img_path=image_path\n",
    "        self.label=label\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_tensor=torch.empty((0,1,3,224,224))\n",
    "        image_list=glob(self.img_path[idx]+'/*.jpg')\n",
    "        image_index=torch.randint(low=0, high=len(image_list),size=(image_count,))\n",
    "        for index in image_index:\n",
    "            image=tf(Image.open(image_list[index]).resize((224,224))).unsqueeze(0)\n",
    "            image_tensor=torch.cat((image_tensor, image.unsqueeze(0)), dim=0)\n",
    "        label_tensor=self.label[idx]\n",
    "        return image_tensor,label_tensor\n",
    "    \n",
    "train_image_transition_path='../../data/Tiling/train/transition/*'\n",
    "train_image_not_transition_path='../../data/Tiling/train/not_transition/*'\n",
    "test_image_transition_path='../../data/Tiling/validation/transition/*'\n",
    "test_image_not_transition_path='../../data/Tiling/validation/not_transition/*'\n",
    "train_image_list=[]\n",
    "train_label_list=[]\n",
    "test_image_list=[]\n",
    "test_label_list=[]\n",
    "train_image_transition_list=glob(train_image_transition_path)\n",
    "train_image_transition_label=torch.ones(len(train_image_transition_list),1)\n",
    "train_image_not_transition_list=glob(train_image_not_transition_path)\n",
    "train_image_not_transition_label=torch.zeros(len(train_image_not_transition_list),1)\n",
    "train_image_list.extend(train_image_transition_list)\n",
    "train_image_list.extend(train_image_not_transition_list)\n",
    "train_label_list.extend(train_image_transition_label)\n",
    "train_label_list.extend(train_image_not_transition_label)\n",
    "test_image_transition_list=glob(test_image_transition_path)\n",
    "test_image_transition_label=torch.ones(len(test_image_transition_list),1)\n",
    "test_image_not_transition_list=glob(test_image_not_transition_path)\n",
    "test_image_not_transition_label=torch.zeros(len(test_image_not_transition_list),1)\n",
    "test_image_list.extend(test_image_transition_list)\n",
    "test_image_list.extend(test_image_not_transition_list)\n",
    "test_label_list.extend(test_image_transition_label)\n",
    "test_label_list.extend(test_image_not_transition_label)\n",
    "\n",
    "    \n",
    "\n",
    "train_dataset=CustomDataset(train_image_list,train_label_list)\n",
    "test_dataset=CustomDataset(test_image_list ,test_label_list)\n",
    "dataset_size = len(test_dataset)\n",
    "test_size = int(dataset_size * 0.5)\n",
    "validation_size = dataset_size-test_size\n",
    "validation_dataset, test_dataset = random_split(test_dataset, [validation_size, test_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_fn(x):\n",
    "    \"\"\" Swish activation function \"\"\"\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "class Conv2dSamePadding(nn.Conv2d):\n",
    "    \"\"\" 2D Convolutions like TensorFlow \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ih, iw = x.size()[-2:]\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, [pad_w//2, pad_w - pad_w//2, pad_h//2, pad_h - pad_h//2])\n",
    "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "    \n",
    "def drop_connect(inputs, p, training):\n",
    "    \"\"\" Drop connect. \"\"\"\n",
    "    if not training: return inputs\n",
    "    batch_size = inputs.shape[0]\n",
    "    keep_prob = 1 - p\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype)  # uniform [0,1)\n",
    "    binary_tensor = torch.floor(random_tensor)\n",
    "    output = inputs.to(device) / keep_prob * binary_tensor.to(device)\n",
    "    return output\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Mobile Inverted Residual Bottleneck Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride, expand_ratio, input_filters, output_filters, se_ratio, drop_n_add):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._bn_mom = 0.1\n",
    "        self._bn_eps = 1e-03\n",
    "        self.has_se = (se_ratio is not None) and (0 < se_ratio <= 1)\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.drop_n_add = drop_n_add\n",
    "\n",
    "        # Filter Expansion phase\n",
    "        inp = input_filters  # number of input channels\n",
    "        oup = input_filters * expand_ratio  # number of output channels\n",
    "        if expand_ratio != 1: # add it except at first block \n",
    "            self._expand_conv = Conv2dSamePadding(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n",
    "            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Depthwise convolution phase\n",
    "        k = kernel_size\n",
    "        s = stride\n",
    "        self._depthwise_conv = Conv2dSamePadding(\n",
    "            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise(conv filter by filter)\n",
    "            kernel_size=k, stride=s, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "\n",
    "        # Squeeze and Excitation layer, if desired\n",
    "        if self.has_se:\n",
    "            num_squeezed_channels = max(1,int(input_filters * se_ratio))  # input channel * 0.25 ex) block2 => 16 * 0.25 = 4\n",
    "            self._se_reduce = Conv2dSamePadding(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n",
    "            self._se_expand = Conv2dSamePadding(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n",
    "\n",
    "        # Output phase\n",
    "        final_oup = output_filters\n",
    "        self._project_conv = Conv2dSamePadding(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n",
    "        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n",
    "    def forward(self, inputs, drop_connect_rate=0.2):\n",
    "    \n",
    "        # Expansion and Depthwise Convolution\n",
    "        x = inputs\n",
    "        if self.expand_ratio != 1:\n",
    "            x = relu_fn(self._bn0(self._expand_conv(inputs)))\n",
    "        x = relu_fn(self._bn1(self._depthwise_conv(x)))\n",
    "\n",
    "        # Squeeze and Excitation\n",
    "        if self.has_se:\n",
    "            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n",
    "            x_squeezed = self._se_expand(relu_fn(self._se_reduce(x_squeezed)))\n",
    "            x = torch.sigmoid(x_squeezed) * x\n",
    "            \n",
    "        # Output phase\n",
    "        x = self._bn2(self._project_conv(x))\n",
    "\n",
    "        # Skip connection and drop connect\n",
    "        if self.drop_n_add == True:\n",
    "            if drop_connect_rate:\n",
    "                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n",
    "            x = x + inputs  # skip connection\n",
    "        return x    \n",
    "    \n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Batch norm parameters\n",
    "        bn_mom = 0.1\n",
    "        bn_eps = 1e-03\n",
    "\n",
    "        # stem\n",
    "        in_channels = 3\n",
    "        out_channels = 32\n",
    "        self._conv_stem = Conv2dSamePadding(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n",
    "        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Build blocks\n",
    "        self._blocks = nn.ModuleList([]) # list 형태로 model 구성할 때\n",
    "        # stage2 r1_k3_s11_e1_i32_o16_se0.25\n",
    "        self._blocks.append(MBConvBlock(kernel_size=3, stride=1, expand_ratio=1, input_filters=32, output_filters=16, se_ratio=0.25, drop_n_add=False))\n",
    "        # stage3 r2_k3_s22_e6_i16_o24_se0.25\n",
    "        self._blocks.append(MBConvBlock(3, 2, 6, 16, 24, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 24, 24, 0.25, True))\n",
    "        # stage4 r2_k5_s22_e6_i24_o40_se0.25\n",
    "        self._blocks.append(MBConvBlock(5, 2, 6, 24, 40, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 40, 40, 0.25, True))\n",
    "        # stage5 r3_k3_s22_e6_i40_o80_se0.25\n",
    "        self._blocks.append(MBConvBlock(3, 2, 6, 40, 80, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 80, 80, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 80, 80, 0.25, True))\n",
    "        # stage6 r3_k5_s11_e6_i80_o112_se0.25\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 80,  112, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 112, 112, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 112, 112, 0.25, True))\n",
    "        # stage7 r4_k5_s22_e6_i112_o192_se0.25\n",
    "        self._blocks.append(MBConvBlock(5, 2, 6, 112, 192, 0.25, False))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 192, 192, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 192, 192, 0.25, True))\n",
    "        self._blocks.append(MBConvBlock(5, 1, 6, 192, 192, 0.25, True))\n",
    "        # stage8 r1_k3_s11_e6_i192_o320_se0.25\n",
    "        self._blocks.append(MBConvBlock(3, 1, 6, 192, 320, 0.25, False))\n",
    "\n",
    "        # Head \n",
    "        in_channels = 320\n",
    "        out_channels = 1280\n",
    "        self._conv_head = Conv2dSamePadding(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self._bn1 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n",
    "\n",
    "        # Final linear layer\n",
    "        self._dropout = 0.2\n",
    "        self._num_classes = 10\n",
    "        self._fc = nn.Linear(out_channels, self._num_classes)\n",
    "        self.fc1=nn.Linear(15680, 1000, bias=False)\n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
    "\n",
    "        # Stem\n",
    "        x = relu_fn(self._bn0(self._conv_stem(inputs)))\n",
    "\n",
    "        # Blocks\n",
    "        for idx, block in enumerate(self._blocks):          \n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Calls extract_features to extract features, applies final linear layer, and returns logits. \"\"\"\n",
    "\n",
    "        # Convolution layers\n",
    "        x = self.extract_features(x.to(device)).to(device)\n",
    "        x = torch.flatten(x, 1).to(device)\n",
    "        x=self.fc1(x).to(device)\n",
    "        return x\n",
    "class Custom_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn=EfficientNet().to(device)\n",
    "        self.hidden_size = 128\n",
    "        self.num_layers = 2\n",
    "        self.rnn = nn.RNN(1000, 128, 2, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "    def forward(self, inputs):\n",
    "        total_x=torch.empty((batch_size,image_count,1000)).to(device)\n",
    "        for i in range(len(inputs)):\n",
    "            for j in range(len(inputs[i])):\n",
    "                x=self.cnn(inputs[i,j].to(device)).to(device)\n",
    "                total_x[i,j]=x.to(device)\n",
    "        h0 = torch.zeros(self.num_layers,total_x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.rnn(total_x, h0)\n",
    "\n",
    "        # 마지막 시퀀스의 출력을 사용하여 예측\n",
    "        out = self.fc(out[:, -1, :]).to(device)\n",
    "       \n",
    "        return out\n",
    "accuracy = torchmetrics.Accuracy(task=\"binary\", num_classes=1).to(device)\n",
    "model = Custom_model()\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "summary(model,(batch_size,image_count,1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_loss=5000\n",
    "train_loss_list=[]\n",
    "val_loss_list=[]\n",
    "train_acc_list=[]\n",
    "val_acc_list=[]\n",
    "for epoch in range(1000):\n",
    "    train=tqdm(train_dataloader)\n",
    "    count=0\n",
    "    running_loss = 0.0\n",
    "    acc_loss=0\n",
    "    for x, y in train:\n",
    "        model.train()\n",
    "        y = y.to(device).float()\n",
    "        count+=1\n",
    "        x=x.to(device).float()\n",
    "        optimizer.zero_grad()  # optimizer zero 로 초기화\n",
    "        predict = model(x).to(device)\n",
    "        cost = criterion(predict, y) # cost 구함\n",
    "        acc=accuracy(predict, y)\n",
    "        cost.backward() # cost에 대한 backward 구함\n",
    "        optimizer.step() \n",
    "        running_loss += cost.item()\n",
    "        acc_loss+=acc\n",
    "        train.set_description(f\"epoch: {epoch+1}/{1000} Step: {count+1} loss : {running_loss/count:.4f} accuracy: {acc_loss/count:.4f}\")\n",
    "    train_loss_list.append((running_loss/count))\n",
    "    train_acc_list.append((acc_loss/count).cpu().detach().numpy())\n",
    "#validation\n",
    "    val=tqdm(validation_dataloader)\n",
    "    model.eval()\n",
    "    count=0\n",
    "    val_running_loss=0.0\n",
    "    acc_loss=0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val:\n",
    "            y = y.to(device).float()\n",
    "            count+=1\n",
    "            x=x.to(device).float()\n",
    "            \n",
    "            predict = model(x).to(device)\n",
    "            acc=accuracy(predict, y)\n",
    "            cost = criterion(predict, y)\n",
    "            val_running_loss+=cost.item()\n",
    "            acc_loss+=acc\n",
    "            val.set_description(f\"Validation epoch: {epoch+1}/{1000} Step: {count+1} loss : {val_running_loss/count:.4f}  accuracy: {acc_loss/count:.4f}\")\n",
    "        val_loss_list.append((val_running_loss/count))\n",
    "        val_acc_list.append((acc_loss/count).cpu().detach().numpy())\n",
    "    if epoch%100==1:\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.subplot(1, 2, 1) \n",
    "        plt.title('loss_graph')\n",
    "        plt.plot(np.arange(epoch+1),train_loss_list,label='train_loss')\n",
    "        plt.plot(np.arange(epoch+1),val_loss_list,label='validation_loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.ylim([0, 1]) \n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)  \n",
    "        plt.title('acc_graph')\n",
    "        plt.plot(np.arange(epoch+1),train_acc_list,label='train_acc')\n",
    "        plt.plot(np.arange(epoch+1),val_acc_list,label='validation_acc')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.ylim([0, 1]) \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    if MIN_loss>(val_running_loss/count):\n",
    "        torch.save(model.state_dict(), '../../model/image_50/EffB0_tile200_RNN_callback.pt')\n",
    "        MIN_loss=(val_running_loss/count)\n",
    "torch.save(model.state_dict(), '../../model/image_50/EffB0_tile200_RNN.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
